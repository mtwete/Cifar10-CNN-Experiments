{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Matthew Twete\n",
        "\n",
        "Experiments with training LeNet architecture CNN on the cifar 10 dataset with various activations and hyperparameters. Including visualizing some feature maps of the best performing LeNet model. Additionally, some hand crafted CNN models were trained on the cifar 10 dataset. The training progess of the models was plotted as well."
      ],
      "metadata": {
        "id": "D8J0gnGzsSCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some code taken/inspiried from https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/ and other sources"
      ],
      "metadata": {
        "id": "JDnC24EasWZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_p71kh8z5qu"
      },
      "outputs": [],
      "source": [
        "#Import libraries\n",
        "import tensorflow.keras\n",
        "import copy\n",
        "from keras import callbacks\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, AveragePooling2D\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Model\n",
        "\n",
        "#Import the dataset\n",
        "(x_train, y_train), (x_test, y_test) = tensorflow.keras.datasets.cifar10.load_data()\n",
        "#Convert labels to one hot encoding\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to normalize test and train data\n",
        "def normalize(test, train):\n",
        "  norm_test = test.astype('float32')\n",
        "  norm_train = train.astype('float32')\n",
        "  norm_test = norm_test/255.0\n",
        "  norm_train = norm_train/255.0\n",
        "  return norm_test,norm_train\n",
        "\n",
        "#Normalize data\n",
        "x_test, x_train = normalize(x_test, x_train)"
      ],
      "metadata": {
        "id": "iXlH8KMRwUSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define class to make a CNN with the LeNet structure\n",
        "class LeNetCNN():\n",
        "    #Constructor, only argument is the type of activation function\n",
        "    def __init__(self,actFunc):\n",
        "      #Build the network\n",
        "      self.network = Sequential()\n",
        "      self.network.add(Conv2D(filters=6, kernel_size=5,strides=1, activation=actFunc, input_shape=(32,32,3)))\n",
        "      self.network.add(AveragePooling2D(pool_size=(2, 2), strides = 2))\n",
        "      self.network.add(Conv2D(filters=16, kernel_size=5,strides=1, activation=actFunc))\n",
        "      self.network.add(AveragePooling2D(pool_size=(2, 2), strides = 2))\n",
        "      self.network.add(Flatten())\n",
        "      self.network.add(Dense(units=120, activation=actFunc))\n",
        "      self.network.add(Dense(units=84, activation=actFunc))\n",
        "      self.network.add(Dense(units=10, activation='softmax'))\n",
        "    #Simple function to get the network summary\n",
        "    def get_summary(self):\n",
        "      print(self.network.summary())\n",
        "    #Wrapper function to train the network, takes the training data and labels, epochs to train for\n",
        "    #batch size, and callback function (needed to get test loss after each epoch)\n",
        "    def fit(self,xdata = None, ydata = None,epoch=1, b_size = 64, callback = None):\n",
        "      return self.network.fit(x = xdata, y = ydata,epochs=epoch,verbose = 2, batch_size =  b_size, callbacks = callback)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Xtwkw0Y3oeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code based on code from https://github.com/keras-team/keras/issues/2548\n",
        "#Class that overrides a function that will run after each training epoch,\n",
        "#this is needed to get the test loss after each epoch\n",
        "class TestEvalCallback(callbacks.Callback):\n",
        "    #Constructor, takes the test data and labels, a list to hold the test loss after each\n",
        "    #epoch and the batch size\n",
        "    def __init__(self, xtest,ytest,test_loss_list, b_size):\n",
        "        self.x_test = xtest\n",
        "        self.y_test = ytest\n",
        "        self.test_loss = test_loss_list\n",
        "        self.batch_size = b_size\n",
        "    #Function that will run after each epoch, it will evaluate the model on the test data, store the\n",
        "    #test loss and print it as well\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        loss = self.model.evaluate(self.x_test, self.y_test,batch_size = self.batch_size, verbose=0)\n",
        "        self.test_loss.append(loss)\n",
        "        print(\"Loss: \", loss)"
      ],
      "metadata": {
        "id": "IE95vgIWdNAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Activation functions\n",
        "sigmoid = \"sigmoid\"\n",
        "tanh = \"tanh\"\n",
        "\n",
        "#Arrays to hold the networks for each activation function\n",
        "#and loss function, there will be three networks in each\n",
        "#one for each learning rate\n",
        "tanhCE_array = []\n",
        "tanhMSE_array = []\n",
        "sigCE_array = []\n",
        "sigMSE_array = []\n",
        "\n",
        "#Learning rates\n",
        "lr = [0.1,0.01,0.001]\n",
        "\n",
        "#Loop 3 times and instantiate networks with the two different activation functions and loss functions,\n",
        "#adding them to their appropriate arrays\n",
        "for learn_rate in lr:\n",
        "  #Instantiate networks\n",
        "  tanhCE = LeNetCNN(tanh)\n",
        "  tanhMSE = LeNetCNN(tanh)\n",
        "  sigCE = LeNetCNN(sigmoid)\n",
        "  sigMSE = LeNetCNN(sigmoid)\n",
        "  #Compile them\n",
        "  tanhCE.network.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=learn_rate),loss=\"CategoricalCrossentropy\")\n",
        "  tanhMSE.network.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=learn_rate),loss=\"MeanSquaredError\")\n",
        "  sigCE.network.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=learn_rate),loss=\"CategoricalCrossentropy\")\n",
        "  sigMSE.network.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=learn_rate),loss=\"MeanSquaredError\")\n",
        "  #Add to the arrays\n",
        "  tanhCE_array.append(tanhCE)\n",
        "  tanhMSE_array.append(tanhMSE)\n",
        "  sigCE_array.append(sigCE)\n",
        "  sigMSE_array.append(sigMSE)\n"
      ],
      "metadata": {
        "id": "90Yqgis-NsDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Variables to hold the number of epochs, batch size and number of learning rates\n",
        "epochs = 15\n",
        "num_learn_rate = 3\n",
        "batch_size = 32\n",
        "#Arrays to hold losses for training and test data for each model\n",
        "tanhCE_losses = [[[],[]],[[],[]],[[],[]]]\n",
        "tanhMSE_losses = [[[],[]],[[],[]],[[],[]]]\n",
        "sigCE_losses = [[[],[]],[[],[]],[[],[]]]\n",
        "sigMSE_losses = [[[],[]],[[],[]],[[],[]]]\n",
        "\n",
        "\n",
        "#Loop over the learning rates and networks with the two different acticvation functions and loss functions\n",
        "for j in range(num_learn_rate):\n",
        "  #Train the tanh CE network with the callback to get the test loss at the end of each epoch\n",
        "  history = tanhCE_array[j].fit(x_train, y_train, epochs, batch_size, callback=[TestEvalCallback(x_test, y_test,tanhCE_losses[j][1],batch_size)])\n",
        "  #Store the training loss over the epochs\n",
        "  tanhCE_losses[j][0] = copy.deepcopy(history.history['loss'])\n",
        "  #Train the tanh MSE network with the callback to get the test loss at the end of each epoch\n",
        "  history = tanhMSE_array[j].fit(x_train, y_train, epochs, batch_size, callback=[TestEvalCallback(x_test, y_test,tanhMSE_losses[j][1],batch_size)])\n",
        "  #Store the training loss over the epochs\n",
        "  tanhMSE_losses[j][0] = copy.deepcopy(history.history['loss'])\n",
        "  #Train the sigmoid CE network with the callback to get the test loss at the end of each epoch\n",
        "  history = sigCE_array[j].fit(x_train, y_train, epochs, batch_size, callback=[TestEvalCallback(x_test, y_test,sigCE_losses[j][1],batch_size)])\n",
        "  #Store the training loss over the epochs\n",
        "  sigCE_losses[j][0] = copy.deepcopy(history.history['loss'])\n",
        "  #Train the sigmoid MSE network with the callback to get the test loss at the end of each epoch\n",
        "  history = sigMSE_array[j].fit(x_train, y_train, epochs, batch_size, callback=[TestEvalCallback(x_test, y_test,sigMSE_losses[j][1],batch_size)])\n",
        "  #Store the training loss over the epochs\n",
        "  sigMSE_losses[j][0] = copy.deepcopy(history.history['loss'])\n",
        "\n"
      ],
      "metadata": {
        "id": "7jo26uC0Q182"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to plot the train & test loss vs epoch for the three networks of a given\n",
        "#activation and loss function\n",
        "def plot_multiple_network_errors(error_array, act_func, loss):\n",
        "  #Set up figure for subplots\n",
        "  plt.figure(figsize=(25, 10))\n",
        "  plt.subplots_adjust(hspace=0.5)\n",
        "  #Add overall title\n",
        "  plt.suptitle(act_func + ' activation function using ' + loss + ' loss CNN models train and test loss vs epoch', fontsize=18, y=0.95)\n",
        "  #Loop over the learning rates\n",
        "  for i in range(num_learn_rate):\n",
        "    #Plot the subplot with a title listing the learning rate\n",
        "    ax = plt.subplot(1,3,i+1)\n",
        "    ax.plot(error_array[i][0])\n",
        "    ax.plot(error_array[i][1])\n",
        "    plt.title('model trained with a learning rate of ' + str(lr[i]))\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    ax.get_yaxis().get_major_formatter().set_useOffset(False)\n",
        "    plt.legend(['train', 'test'], loc='lower right')\n",
        "\n",
        "#Plot the graphs for networks with each of the activation and loss functions\n",
        "plot_multiple_network_errors(tanhCE_losses, 'Tanh','Cross-Entropy')\n",
        "plot_multiple_network_errors(tanhMSE_losses, 'Tanh','Mean Squared Error')\n",
        "plot_multiple_network_errors(sigCE_losses, 'Sigmoid','Cross-Entropy')\n",
        "plot_multiple_network_errors(sigMSE_losses, 'Sigmoid','Mean Squared Error')"
      ],
      "metadata": {
        "id": "caqe56vThIbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display feature maps, I will use the first 10 training data points as the 10 images\n",
        "\n",
        "#I choose the sigmoid CE network trained with a learning rate of 0.001\n",
        "model = sigCE_array[2].network\n",
        "\n",
        "\n",
        "\n",
        "#Redfine a model to output after the second convolution layer\n",
        "model = Model(inputs=model.input, outputs=model.layers[2].output)\n",
        "\n",
        "\n",
        "#Now plot the feature maps of first 10 training data points\n",
        "for i in range(10):\n",
        "  #Print the image number\n",
        "  print(\"Image #\", i+1)\n",
        "  #Plot the training data points in its original form for comparison\n",
        "  print(\"Original Image\") \n",
        "  plt.imshow(x_train[i])\n",
        "  plt.show()\n",
        "  #Now get the feature map for the image\n",
        "  img = copy.deepcopy(x_train[i])\n",
        "  img = img[None,:,:,:]\n",
        "  feature_maps = model.predict(img)\n",
        "\n",
        "  # plot all 16 maps in 4x4 squares\n",
        "  print(\"Feature Maps\")\n",
        "  square = 4\n",
        "  ix = 1\n",
        "  for _ in range(square):\n",
        "    for _ in range(square):\n",
        "      #Specify subplot and turn of axis\n",
        "      ax = plt.subplot(square, square, ix)\n",
        "      ax.set_xticks([])\n",
        "      ax.set_yticks([])\n",
        "      #Plot filter channel in grayscale\n",
        "      plt.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
        "      ix += 1\n",
        "  #Show the figure\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "XUP3x_dZqSQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define relu model\n",
        "actFunc = 'relu'\n",
        "network = Sequential()\n",
        "network.add(Conv2D(filters=6, kernel_size=3,strides=1, activation=actFunc, input_shape=(32,32,3)))\n",
        "network.add(AveragePooling2D(pool_size=(2, 2), strides = 2))\n",
        "network.add(Conv2D(filters=16, kernel_size=3,strides=1, activation=actFunc))\n",
        "network.add(AveragePooling2D(pool_size=(2, 2), strides = 2))\n",
        "network.add(Flatten())\n",
        "network.add(Dense(units=120, activation=actFunc))\n",
        "network.add(Dense(units=84, activation=actFunc))\n",
        "network.add(Dense(units=10, activation='softmax'))\n",
        "network.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.001),loss=\"CategoricalCrossentropy\")\n",
        "\n",
        "#Array to hold test loss after each epoch\n",
        "relu_test_error = []\n",
        "\n",
        "#Train the network\n",
        "epochs = 20\n",
        "history = network.fit(x = x_train, y =  y_train,epochs = epochs, verbose = 2, batch_size = batch_size,callbacks=[TestEvalCallback(x_test, y_test,relu_test_error,batch_size)])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ijxljxCuqSeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define plotting function for relu model\n",
        "def plot_relu_network_errors(error_array, act_func, loss,lr):\n",
        "  plt.figure(figsize=(15, 10))\n",
        "  plt.title(act_func + ' activation function using ' + loss + ' loss 3x3 kernel CNN model with a learning rate of ' +str(lr)+ ' train and test loss vs epoch')\n",
        "  plt.plot(error_array[0])\n",
        "  plt.plot(error_array[1])\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='lower right')\n",
        "  plt.show()\n",
        "  \n",
        "#Plot the results\n",
        "plot_relu_network_errors([history.history['loss'],relu_test_error],'ReLU','Cross-Entropy',0.001)"
      ],
      "metadata": {
        "id": "BVAgXZ4yuEeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define model a final model, I will use an ReLU activation, with cross-entropy loss and a learning rate of 0.001\n",
        "actFunc = 'relu'\n",
        "deep_network = Sequential()\n",
        "deep_network.add(Conv2D(filters=6, kernel_size=3,strides=1, activation=actFunc, input_shape=(32,32,3),padding = \"same\"))\n",
        "deep_network.add(AveragePooling2D(pool_size=(2, 2), strides = 1,padding = \"same\"))\n",
        "deep_network.add(Conv2D(filters=16, kernel_size=3,strides=1, activation=actFunc,padding = \"same\"))\n",
        "deep_network.add(AveragePooling2D(pool_size=(2, 2), strides = 1,padding = \"same\"))\n",
        "deep_network.add(Conv2D(filters=16, kernel_size=3,strides=1, activation=actFunc,padding = \"same\"))\n",
        "deep_network.add(AveragePooling2D(pool_size=(2, 2), strides = 1,padding = \"same\"))\n",
        "deep_network.add(Conv2D(filters=16, kernel_size=3,strides=1, activation=actFunc,padding = \"same\"))\n",
        "deep_network.add(AveragePooling2D(pool_size=(2, 2), strides = 1,padding = \"same\"))\n",
        "deep_network.add(Conv2D(filters=16, kernel_size=3,strides=1, activation=actFunc,padding = \"same\"))\n",
        "deep_network.add(AveragePooling2D(pool_size=(2, 2), strides = 1,padding = \"same\"))\n",
        "deep_network.add(Flatten())\n",
        "deep_network.add(Dense(units=120, activation=actFunc))\n",
        "deep_network.add(Dense(units=84, activation=actFunc))\n",
        "deep_network.add(Dense(units=10, activation='softmax'))\n",
        "deep_network.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.001),loss=\"CategoricalCrossentropy\")\n",
        "\n",
        "#Array to hold test loss after each epoch\n",
        "deep_network_test_error = []\n",
        "\n",
        "#Train the network\n",
        "epochs = 20\n",
        "history = deep_network.fit(x = x_train, y =  y_train,epochs = epochs, verbose = 2, batch_size = batch_size,callbacks=[TestEvalCallback(x_test, y_test,deep_network_test_error,batch_size)])"
      ],
      "metadata": {
        "id": "C__ZjK8WcF0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define plotting function for final model\n",
        "def plot_deep_network_errors(error_array, act_func, loss,lr):\n",
        "  plt.figure(figsize=(15, 10))\n",
        "  plt.title(act_func + ' activation function using ' + loss + ' loss 3x3 kernel 5 layer CNN model with a learning rate of ' +str(lr)+ ' train and test loss vs epoch')\n",
        "  plt.plot(error_array[0])\n",
        "  plt.plot(error_array[1])\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'test'], loc='lower right')\n",
        "  plt.show()\n",
        "  \n",
        "#Plot the results\n",
        "plot_deep_network_errors([history.history['loss'],deep_network_test_error],'ReLU','Cross-Entropy',0.001)"
      ],
      "metadata": {
        "id": "azh38sEWf3L4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}